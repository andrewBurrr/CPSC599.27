{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import explacy\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cltk.stop.greek.stops import STOPS_LIST\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "from cltk.corpus.greek.alphabet import expand_iota_subscript"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_qualified_pairs(xml_paths):\n",
    "    qualified_pairs = []\n",
    "    for xml_path in xml_paths:\n",
    "        match = re.search(r\"(?P<name>[^_]*)_(?P<language>gk|eng).(?P<extension>xml)\", xml_path)\n",
    "        if match is None: continue\n",
    "        if match.group(\"language\") == \"eng\":\n",
    "            gk_file = xml_path.replace(\"eng\", \"gk\")\n",
    "            if os.path.isfile(gk_file):\n",
    "                qualified_pairs.append(gk_file)\n",
    "                qualified_pairs.append(xml_path)\n",
    "    return qualified_pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def perseus_tei_xml_to_text():\n",
    "    xml_dir = os.path.normpath(\"./data/Classics/*/*/*.xml\")\n",
    "    xml_paths = glob.glob(xml_dir)\n",
    "\n",
    "    xml_paths = get_qualified_pairs(xml_paths)\n",
    "    new_dir = os.path.normpath(\"./data/Pairs/\")\n",
    "    if not os.path.isdir(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    for xml_path in xml_paths:\n",
    "        xml_names = os.path.split(xml_path)\n",
    "        xml_name = xml_names[1].rstrip(\".xml\")\n",
    "        xml_name += \".txt\"\n",
    "\n",
    "        with open(xml_path) as file_open:\n",
    "            soup = BeautifulSoup(file_open, \"lxml\")\n",
    "        title = soup.title\n",
    "        author = soup.author\n",
    "\n",
    "        tei_header = soup.find('teiHeader')\n",
    "        if tei_header:\n",
    "            tei_header.decompose()\n",
    "\n",
    "        for tag in soup(['lb', 'pb', 'lpar', 'rpar']):\n",
    "            tag.decompose()\n",
    "\n",
    "        body = soup.body\n",
    "        text = body.get_text()\n",
    "        new_plain_text_path = os.path.join(new_dir, xml_name)\n",
    "        with open(new_plain_text_path, \"w\") as file_open:\n",
    "            file_open.write(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "perseus_tei_xml_to_text()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Documents/School/Resources/Environments/CPSC599.27/lib/python3.9/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'grc_ud_proiel_trf' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data_dir, src_lang, tgt_lang, src_model, tgt_model):\n",
    "        r = Replacer()\n",
    "        self.data = []\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(f\"{src_lang}.txt\"):\n",
    "                tgt_file = file.replace(src_lang, tgt_lang)\n",
    "                if tgt_file in os.listdir(data_dir):\n",
    "                    with open(os.path.join(data_dir, file), \"r\") as f_src, open(os.path.join(data_dir, tgt_file), \"r\") as f_tgt:\n",
    "                        src_txt = r.beta_code(r\"%s\", f_src.read().strip())\n",
    "                        src_sents = [sent.text.strip() for doc in src_model(str(src_txt)) for sent in doc.sents]\n",
    "                        tgt_sents = [sent.text.strip() for doc in tgt_model(f_src.read().strip()) for sent in doc.sents]\n",
    "                        for src_sent, tgt_sent in zip(src_sents, tgt_sents):\n",
    "                            self.data.append((src_sent, tgt_sent))\n",
    "\n",
    "        self.src_model = src_model\n",
    "        self.tgt_model = tgt_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_sent, tgt_sent = self.data[index]\n",
    "\n",
    "        src_tokens = [token.text for token in self.src_model(src_sent)]\n",
    "        tgt_tokens = [token.text for token in self.tgt_model(tgt_sent)]\n",
    "\n",
    "        src_tokens = [\"<s>\"] + src_tokens + [\"</s>\"]\n",
    "        tgt_tokens = [\"<s>\"] + tgt_tokens + [\"</s>\"]\n",
    "\n",
    "        src_ids = torch.tensor(self.src_model.convert_tokens_to_ids(src_tokens))\n",
    "        tgt_ids = torch.tensor(self.tgt_model.convert_tokens_to_ids(tgt_tokens))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_ids,\n",
    "            \"attention_mask\": torch.ones(len(src_ids)),\n",
    "            \"decoder_input_ids\": tgt_ids[:-1],\n",
    "            \"decoder_attention_mask\": torch.ones(len(tgt_ids)-1),\n",
    "            \"labels\": tgt_ids[1:]\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data_dir, source_lang, target_lang, nlp_src, nlp_tgt, max_seq_len):\n",
    "        self.nlp_source = nlp_src\n",
    "        self.nlp_target = nlp_tgt\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Read file pairs from the data directory\n",
    "        self.file_pairs = self.read_file_pairs(data_dir, source_lang, target_lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        r = Replacer()\n",
    "        file_pair = self.file_pairs[index]\n",
    "        with open(file_pair[\"source\"]) as f_src, open(file_pair[\"target\"], \"r\") as f_tgt:\n",
    "            source_text = r.beta_code(r\"%s\" % f_src.read())\n",
    "            target_text = f_tgt.read()\n",
    "\n",
    "        # Tokenize the source and target sentences\n",
    "        source_tokens = self.preprocess(source_text, self.nlp_source)\n",
    "        target_tokens = self.preprocess(target_text, self.nlp_target)\n",
    "        # Add special tokens to the input and output sequences\n",
    "        input_ids = [self.nlp_source.vocab.strings[source_token.text] for source_token in source_tokens]\n",
    "        labels = [self.nlp_target.vocab.strings[target_token.text] for target_token in target_tokens]\n",
    "\n",
    "        # Pad or truncate the input and output sequences to max_seq_len\n",
    "        if len(input_ids) > self.max_seq_len:\n",
    "            input_ids = input_ids[:self.max_seq_len]\n",
    "        else:\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "        if len(labels) > self.max_seq_len:\n",
    "            labels = labels[:self.max_seq_len]\n",
    "        else:\n",
    "            labels = labels + [0] * (self.max_seq_len - len(labels))\n",
    "\n",
    "        # Create attention masks to ignore padded tokens\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Convert to PyTorch tensors and return as dictionary\n",
    "        return {\n",
    "            \"input_ids\": torch.LongTensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"labels\": torch.LongTensor(labels),\n",
    "        }\n",
    "\n",
    "    def read_file_pairs(self, data_dir, source_lang, target_lang):\n",
    "        file_pairs = []\n",
    "        for filename in os.listdir(data_dir):\n",
    "            if filename.endswith(f\"{source_lang}.txt\"):\n",
    "                source_file = filename\n",
    "                target_file = filename.replace(f\"_{source_lang}\", f\"_{target_lang}\")\n",
    "                file_pairs.append({\"source\": data_dir + \"/\" + source_file, \"target\": data_dir + \"/\" + target_file})\n",
    "        return file_pairs\n",
    "\n",
    "    def preprocess(self, text, nlp):\n",
    "        doc = nlp(text)\n",
    "        return [token for token in doc.sents][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Documents/School/Resources/Environments/CPSC599.27/lib/python3.9/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'grc_ud_proiel_md' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Overflow when unpacking long",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[123], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data/Pairs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m translation_dataset \u001B[38;5;241m=\u001B[39m TranslationDataset(data_dir, src_lang, tgt_lang, nlp_grc, nlp_eng, \u001B[38;5;241m512\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[43mtranslation_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# batch_size = 8\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# print('Training complete!')\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[122], line 42\u001B[0m, in \u001B[0;36mTranslationDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     38\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(input_ids)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Convert to PyTorch tensors and return as dictionary\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLongTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(attention_mask),\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mLongTensor(labels),\n\u001B[1;32m     45\u001B[0m }\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Overflow when unpacking long"
     ]
    }
   ],
   "source": [
    "# model_name = \"t5-small\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "src_lang = \"gk\"\n",
    "tgt_lang = \"eng\"\n",
    "nlp_grc = spacy.load(\"grc_ud_proiel_md\")\n",
    "nlp_eng = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "data_dir = \"./data/Pairs\"\n",
    "translation_dataset = TranslationDataset(data_dir, src_lang, tgt_lang, nlp_grc, nlp_eng, 512)\n",
    "translation_dataset.__getitem__(1)\n",
    "#\n",
    "# batch_size = 8\n",
    "#\n",
    "# data_loader = DataLoader(translation_dataset, batch_size=batch_size, shuffle=True)\n",
    "#\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "# num_epochs = 5\n",
    "#\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch in data_loader:\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['labels']\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print('Epoch:', epoch, 'Batch loss:', loss.item())\n",
    "#\n",
    "#     # Save the model after each epoch\n",
    "#     save_dir = f'./saved_models/t5_{epoch}.pt'\n",
    "#     torch.save(model.state_dict(), save_dir)\n",
    "#\n",
    "# print('Training complete!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def clean_doc(raw_txt):\n",
    "#     with open(raw_txt, \"r\") as file:\n",
    "#         text = file.read()\n",
    "#     doc = nlp(text)\n",
    "#     for sent in doc.sents:\n",
    "#         words = [ word for word in sent if not word in STOPS_LIST ]\n",
    "print(sample:=[*doc.sents][150])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree             Token      Dep type  Lemma      Part of Sp\n",
      "──────────────────── ────────── ───────── ────────── ──────────\n",
      "         ┌─────────► πνοαὶ      nsubj     πνοή       NOUN      \n",
      "         │┌────────► δ’         discourse δ’         ADV       \n",
      "         ││      ┌─► ἀπὸ        case      ἀπό        ADP       \n",
      "         ││   ┌─►└── Στρυμόνος  nmod      Στρυμών    PROPN     \n",
      "         ││┌─►└───── μολοῦσαι   advcl     βλώσκω     VERB      \n",
      "┌───────►└┴┴──────── \n",
      "          advcl     \n",
      "          CCONJ     \n",
      "│          ┌─►┌┬┬┬── κακόσχολοι amod      κακόσχολος ADJ       \n",
      "│          │  │││└─► νήστιδες   conj      νήστις     ADJ       \n",
      "│          │  ││└──► δύσορμοι   conj      δύσορμος   ADJ       \n",
      "│          │  │└───► ,          cc        ,          PUNCT     \n",
      "│          │  └─►┌── \n",
      "          conj      \n",
      "          CCONJ     \n",
      "│          │     └─► βροτῶν     nmod      βροτός     NOUN      \n",
      "│   ┌─►┌┬──┴─────┬── ἄλαι       obj       ἄλαι       ADJ       \n",
      "│   │  ││        └─► ,          dep       ,          PUNCT     \n",
      "│   │  ││        ┌─► ναῶν       nmod      ναός       NOUN      \n",
      "│   │  │└─►┌─────┼── <          dep       <          NOUN      \n",
      "│   │  │   │     └─► τε         cc        τε         CCONJ     \n",
      "│   │  │   └─►┌─┬┬── >          conj      >          ADJ       \n",
      "│   │  │      │ │└─► καὶ        cc        καί        CCONJ     \n",
      "│   │  │      │ └──► \n",
      "          conj      \n",
      "          CCONJ     \n",
      "│   │  │      │  ┌─► πεισμάτων  nmod      πεισμάτων  NOUN      \n",
      "│   │  │      └─►└── ἀφειδεῖς   conj      ἀφειδής    ADJ       \n",
      "│   │  │         ┌─► ,          nmod      ,          PUNCT     \n",
      "│   │  └────────►└── \n",
      "          dep       \n",
      "          CCONJ     \n",
      "│   │           ┌──► παλιμμήκη  xcomp     παλιμμήκης ADJ       \n",
      "│   │           │┌─► χρόνον     obj       χρόνος     NOUN      \n",
      "│┌─►└───────────┴┼── τιθεῖσαι   advcl     τιθεῖσαι   VERB      \n",
      "││               └─► \n",
      "          obj       \n",
      "          CCONJ     \n",
      "││               ┌─► τρίβῳ      obl       τρίβος     NOUN      \n",
      "└┴─────────────┬┬┼── κατέξαινον ROOT      κατέξαινον VERB      \n",
      "               ││└─► ἄν         advmod    ἄν         ADV       \n",
      "               │└──► \n",
      "          obj       \n",
      "          CCONJ     \n",
      "               └───► θος        obj       θος        NOUN      \n",
      "                 ┌─► Ἀργείων    nmod      Ἀργεῖος    ADJ       \n",
      "                 └── ·          ROOT      ·          ADV       \n"
     ]
    }
   ],
   "source": [
    "explacy.print_parse_info(nlp, u\"%s\" % sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latin_text_perseus', 'latin_treebank_perseus', 'latin_text_latin_library', 'phi5', 'phi7', 'latin_proper_names_cltk', 'latin_models_cltk', 'latin_pos_lemmata_cltk', 'latin_treebank_index_thomisticus', 'latin_lexica_perseus', 'latin_training_set_sentence_cltk', 'latin_word2vec_cltk', 'latin_text_antique_digiliblt', 'latin_text_corpus_grammaticorum_latinorum', 'latin_text_poeti_ditalia', 'latin_text_tesserae']\n"
     ]
    }
   ],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "corpus_importer = CorpusImporter(\"latin\")\n",
    "print(corpus_importer.list_corpora)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100% 9.40 MiB | 11.80 MiB/s \r"
     ]
    }
   ],
   "source": [
    "corpus_importer.import_corpus(\"latin_treebank_perseus\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'property' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_corpus_reader\n\u001B[1;32m      2\u001B[0m greek_corpus \u001B[38;5;241m=\u001B[39m get_corpus_reader(corpus_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatin_text_perseus\u001B[39m\u001B[38;5;124m\"\u001B[39m, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/School/Resources/Environments/CPSC599.27/lib/python3.9/site-packages/cltk/corpus/readers.py:18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pos_tag  \u001B[38;5;66;03m# Replace with CLTK\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprosody\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlatin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstring_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m flatten\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msentence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TokenizeSentence\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mword\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordTokenizer\n\u001B[1;32m     21\u001B[0m LOG \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/School/Resources/Environments/CPSC599.27/lib/python3.9/site-packages/cltk/tokenize/sentence.py:15\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpunkt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PunktLanguageVars\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpunkt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PunktSentenceTokenizer\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlatin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparams\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LatinLanguageVars\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgreek\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparams\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GreekLanguageVars\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msanskrit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparams\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SanskritLanguageVars\n",
      "File \u001B[0;32m~/Documents/School/Resources/Environments/CPSC599.27/lib/python3.9/site-packages/cltk/tokenize/latin/params.py:155\u001B[0m\n\u001B[1;32m    129\u001B[0m latin_exceptions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(que_exceptions\n\u001B[1;32m    130\u001B[0m                             \u001B[38;5;241m+\u001B[39m ne_exceptions\n\u001B[1;32m    131\u001B[0m                             \u001B[38;5;241m+\u001B[39m n_exceptions\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    134\u001B[0m                             \u001B[38;5;241m+\u001B[39m st_exceptions\n\u001B[1;32m    135\u001B[0m                             ))\n\u001B[1;32m    137\u001B[0m latin_replacements \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    138\u001B[0m     (\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbmecum\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcum me\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    139\u001B[0m     (\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbtecum\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcum te\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m     (\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbqualist\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqualis est\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    153\u001B[0m ]\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLatinLanguageVars\u001B[39;00m(PunktLanguageVars):\n\u001B[1;32m    156\u001B[0m     _re_non_word_chars \u001B[38;5;241m=\u001B[39m PunktLanguageVars\u001B[38;5;241m.\u001B[39m_re_non_word_chars\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    158\u001B[0m PUNCTUATION \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/School/Resources/Environments/CPSC599.27/lib/python3.9/site-packages/cltk/tokenize/latin/params.py:156\u001B[0m, in \u001B[0;36mLatinLanguageVars\u001B[0;34m()\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLatinLanguageVars\u001B[39;00m(PunktLanguageVars):\n\u001B[0;32m--> 156\u001B[0m     _re_non_word_chars \u001B[38;5;241m=\u001B[39m \u001B[43mPunktLanguageVars\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_re_non_word_chars\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'property' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "from cltk.corpus.readers import get_corpus_reader\n",
    "greek_corpus = get_corpus_reader(corpus_name=\"latin_text_perseus\", language=\"latin\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('θεοὺς μὲν αἰτῶ τῶνδ’ ἀπαλλαγὴν πόνων\\nφρουρᾶς ἐτείας μῆκος, ἣν κοιμώμενοσ\\nστέγαις Ἀτρειδῶν ἄγκαθεν, κυνὸς δίκην,\\nἄστρων κάτοιδα νυκτέρων ὁμήγυριν,\\nκαὶ τοὺς φέροντας χεῖμα καὶ θέρος βροτοῖσ\\nλαμπροὺς δυνάστας, ἐμπρέποντας αἰθέρι\\nἀστέρας, ὅταν φθίνωσιν, ἀντολάς τε τῶν.\\n καὶ νῦν φυλάσσω λαμπάδος τό σύμβολον,\\nαὐγὴν πυρὸς φέρουσαν ἐκ Τροίας φάτιν\\nἁλώσιμόν τε βάξιν· ὧδε γὰρ κρατεῖ\\nγυναικὸς ἀνδρόβουλον ἐλπίζον κέαρ.\\nεὖτ’ ἂν δὲ νυκτίπλαγκτον ἔνδροσόν τ’ ἔχω\\nεὐνὴν ὀνείροις οὐκ ἐπισκοπουμένην\\nἐμήν· φόβος γὰρ ἀνθ’ ὕπνου παραστατεῖ,\\nτὸ μὴ βεβαίως βλέφαρα συμβαλεῖν ὕπνῳ·\\nὅταν δ’ ἀείδειν ἢ μινύρεσθαι δοκῶ,\\nὕπνου τόδ’ ἀντίμολπον ἐντέμνων ἄκος,\\nκλαίω τότ’ οἴκου τοῦδε συμφορὰν στένων\\nοὐχ ὡς τὰ πρόσθ’ ἄριστα διαπονουμένου.\\nνῦν δ’ εὐτυχὴς γένοιτ’ ἀπαλλαγὴ πόνων\\nεὐαγγέλου φανέντος ὀρφναίου πυρός.\\n ὦ χαῖρε λαμπτὴρ νυκτός, ἡμερήσιον\\nφάος πιφαύσκων καὶ χορῶν κατάστασιν\\nπολλῶν ἐν Ἄργει, τῆσδε συμφορᾶς χάριν.\\nἰοὺ ἰού.\\n Ἀγαμέμνονος γυναικὶ σημαίνω τορῶσ\\nεὐνῆς ἐπαντείλασαν ὡς τάχος δόμοισ\\nὀλολυγμὸν εὐφημοῦντα τῇδε λαμπάδι\\nἐπορθιάζειν, εἴπερ Ἰλίου πόλισ\\nἑάλωκεν, ὡς ὁ φρυκτὸς ἀγγέλλων πρέπει·\\nαὐτός τ’ ἔγωγε φροίμιον χορεύσομαι.\\nτὰ δεσποτῶν γὰρ εὖ πεσόντα θήσομαι\\nτρὶς ἓξ βαλούσης τῆσδέ μοι φρυκτωρίας.\\n γένοιτο δ’ οὖν μολόντος εὐφιλῆ χέρα\\nἄνακτος οἴκων τῇδε βαστάσαι χερί.\\nτὰ δ’ ἄλλα σιγῶ· βοῦς ἐπὶ γλώσσῃ μέγασ\\nβέβηκεν· οἶκος δ’ αὐτός, εἰ φθογγὴν λάβοι,\\nσαφέστατ’ ἂν λέξειεν· ὡς ἑκὼν ἐγὼ\\nμαθοῦσιν αὐδῶ κοὐ μαθοῦσι λήθομαι.', 'Upon the roof of the palace of Agamemnon at Argos')\n"
     ]
    }
   ],
   "source": [
    "from cltk.corpus.greek.beta_to_unicode import Replacer\n",
    "import spacy\n",
    "\n",
    "r = Replacer()\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# sen_tokenizer = SentenceTokenizer()\n",
    "split_pattern = re.compile(r\"\\n{2,}\\s*\")\n",
    "with open(\"./data/Pairs/aesch.ag_gk.txt\") as grc_f, open(\"./data/Pairs/aesch.ag_eng.txt\") as eng_f:\n",
    "    gk_text = r.beta_code(r\"%s\" % grc_f.read())\n",
    "    eng_text = eng_f.read()\n",
    "\n",
    "gk_pars = split_pattern.split(gk_text)\n",
    "en_pars = split_pattern.split(eng_text)\n",
    "pair = list(zip(gk_pars, en_pars))\n",
    "print(pair[1])\n",
    "# # gr_sents = sen_tokenizer.tokenize(gk_text)\n",
    "# en_sents = list(doc.sents)\n",
    "# gk_sents = list(g_doc.sents)\n",
    "# print(f\"gr{len(gk_sents)}\")\n",
    "# print(f\"en{len(en_sents)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
